{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38f40668",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03457b1",
   "metadata": {},
   "source": [
    "When you get data it is sometimes not in the right format and before you can input data to your Machine Learning Models you have to prepare it (clean the data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e028152",
   "metadata": {},
   "source": [
    "## Pre-Processing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f508b30",
   "metadata": {},
   "source": [
    "### Steps fopr Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6beca1",
   "metadata": {},
   "source": [
    "## Step 1: Import Useful packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78dc736c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import sklearn.preprocessing as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf5d5f7",
   "metadata": {},
   "source": [
    "## Step 2: Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c59b1af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are going to create a sample array\n",
    "input_data = np.array([[2.1, -1.4, 3.2], [3.2, 1.4,  4.0], [1.3, 4.3, 2.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d699127",
   "metadata": {},
   "source": [
    "## Step 3: Applying Pre-Processing Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de58d23",
   "metadata": {},
   "source": [
    "### Techniques for Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7517b410",
   "metadata": {},
   "source": [
    "## Binarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e722d4",
   "metadata": {},
   "source": [
    "\n",
    "This is a preprocesing technique where we turn our numerical values into boolean((yes/no), (1/0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f308d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binarized data:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binarizer = sp.Binarizer().fit(input_data)\n",
    "\n",
    "print(\"Binarized data:\")\n",
    "binarizer.transform(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cb210b",
   "metadata": {},
   "source": [
    "## Mean Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb46b72d",
   "metadata": {},
   "source": [
    "This is a preprocessing technique where you remove the mean from every feature vector so that every feature will thenhave zero as their mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8da98175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: [2.2        1.43333333 3.06666667]\n",
      "std_dev: [0.7788881  2.32713462 0.82192187]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#we will find the meand and standard deviation of the inout_data array, and output them. \n",
    "\n",
    "mean = input_data.mean(axis=0)\n",
    "\n",
    "std_deviation = input_data.std(axis=0)\n",
    "\n",
    "print(\"mean:\", str(mean))\n",
    "\n",
    "print(\"std_dev:\", str(std_deviation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d1d47f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaled data mean: [-7.40148683e-17  0.00000000e+00  3.70074342e-16]\n",
      "scaled data std_deviation: [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "#Now we will remove the mean and Standardd deviation of the input_data array\n",
    "scaled_data = sp.scale(input_data)\n",
    "\n",
    "#now the mean of the input_data array have been removed and saved to the variable scaled_data\n",
    "#see proof below\n",
    "mean = scaled_data.mean(axis=0)\n",
    "\n",
    "std_deviation = scaled_data.std(axis=0)\n",
    "\n",
    "print(\"scaled data mean:\", str(mean))\n",
    "\n",
    "print(\"scaled data std_deviation:\", str(std_deviation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c659c1ad",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa47cad",
   "metadata": {},
   "source": [
    "Scaling (AKA as Feature Scaling) is simply the act of normalizing the range of values in the data set. In simple words, some values in the data array are sometimes too big or too small and so feature scaling helps to make the data look organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8605bfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Max Scaled Data: [[0.42105263 0.         0.6       ]\n",
      " [1.         0.49122807 1.        ]\n",
      " [0.         1.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Min Max scaling\n",
    "\n",
    "#here we are providing the range which the data will be scaled\n",
    "data_scaler_minmax = sp.MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "#now we are fitting the scaled minmax with the input_data array\n",
    "data_scaled_minmax = data_scaler_minmax.fit_transform(input_data)\n",
    "\n",
    "#now we output our scaled data\n",
    "print(\"Min Max Scaled Data:\", str(data_scaled_minmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cb6c96",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ebf581",
   "metadata": {},
   "source": [
    "This is another preprocessing techique which is used to preprocess data. It is used when you want to measure the feature vectors on a common scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4bffaf",
   "metadata": {},
   "source": [
    "\n",
    "There are 2 types of Normalization techniques in Machine Learnig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58386d30",
   "metadata": {},
   "source": [
    "### L1 Normalization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8bb878",
   "metadata": {},
   "source": [
    "\n",
    "This is also know as Least Absolute Deviations. This modifies the values so that the sum of the absolute values is always up to one in each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf2e662e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 normalized data: [[ 0.31343284 -0.20895522  0.47761194]\n",
      " [ 0.37209302  0.1627907   0.46511628]\n",
      " [ 0.17105263  0.56578947  0.26315789]]\n"
     ]
    }
   ],
   "source": [
    "#L1 Normalization implementation \n",
    "\n",
    "L1_normalized_data = sp.normalize(input_data, norm='l1')\n",
    "\n",
    "#output L1 normalized data\n",
    "print(\"L1 normalized data:\", str(L1_normalized_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafe0d99",
   "metadata": {},
   "source": [
    "### L2 Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014c297d",
   "metadata": {},
   "source": [
    "This is also known as Least squares, this kind of normalization modifies the values so that the sum of the squares of the squares is always up to one on each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "954c90bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 Normalized data: [[ 0.51526955 -0.34351303  0.78517265]\n",
      " [ 0.60259486  0.26363525  0.75324358]\n",
      " [ 0.26437185  0.87446072  0.40672592]]\n"
     ]
    }
   ],
   "source": [
    "#L2 Normalization Implementation\n",
    "L2_normalized_data = sp.normalize(input_data, norm = 'l2')\n",
    "\n",
    "#output L2 normalized data\n",
    "print(\"L2 Normalized data:\", str(L2_normalized_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104cdc60",
   "metadata": {},
   "source": [
    "# Labelling of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbd941b",
   "metadata": {},
   "source": [
    "In machine Learning data has to be in a certain format before it can be inputed to a machine learning model. In Classification for instance, there are a lot of labels on the data and for an Machine Learning algorithm to recognize those labels the labels need to be in numeric form and if they are not in numeric for they need to be changed and this leads to label Encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dffaf5",
   "metadata": {},
   "source": [
    "## Steps Involved in Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a569ff",
   "metadata": {},
   "source": [
    "### Step 1: Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c93bcdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc2bbfa",
   "metadata": {},
   "source": [
    "### Step 2: Defining Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e72c5e",
   "metadata": {},
   "source": [
    "After importing the packages we need to define some labels so that we can create and train our label encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08f024d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample Input Labels\n",
    "\n",
    "input_labels = ['dog', 'cat', 'bird', 'insects', 'ape', 'snake', 'lizard']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2c77c9",
   "metadata": {},
   "source": [
    "### Step 3: Creating and Training of Label Encoder Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61ce4c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the label enoder\n",
    "\n",
    "encoder = sp.LabelEncoder()\n",
    "\n",
    "encoder.fit(input_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea7f41b",
   "metadata": {},
   "source": [
    "### Step 4: Checking the performance by encoding a random ordered list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69b0bd7",
   "metadata": {},
   "source": [
    "In this step we check the ability of our Label Encoder by making it encode a couple of test_labels. You have to make sure you test the encoder label on labels it has already been trained on in our case -dog, cats, etc. if you test it on anything else you'll get an error message saying \"unseen elements label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "791b3c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Test Label:\n",
      "[6 3 2]\n"
     ]
    }
   ],
   "source": [
    "test_labels = ['snake', 'dog', 'cat']\n",
    "\n",
    "encoded_values = encoder.transform(test_labels)\n",
    "\n",
    "print(\"Encoded Test Label:\")\n",
    "print(encoded_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce23e8e",
   "metadata": {},
   "source": [
    "### Step 5: Checking the Performance by decoding a random set of numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1b695e",
   "metadata": {},
   "source": [
    "We checked the ability of our label encoder earlier on encoding labels now let's see it's ability on decoding a random set of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4338b6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded_Labels:\n",
      "['dog' 'ape' 'insects' 'bird']\n"
     ]
    }
   ],
   "source": [
    "#we provide a set of encoded values to be decoded\n",
    "encoded_values = [3, 0, 4, 1]\n",
    "\n",
    "#Now we decode the list\n",
    "decoded_list = encoder.inverse_transform(encoded_values)\n",
    "\n",
    "print(\"Decoded_Labels:\")\n",
    "print(decoded_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121dab7b",
   "metadata": {},
   "source": [
    "## Labelled vs Unlabelled Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7805b0e",
   "metadata": {},
   "source": [
    "Unlabelled data consists mostly of the samples of the samples of natural or human created object that can easily be obtained from the world they include: audio, video, photos, news articles\n",
    "\n",
    "Labelled data is simply data with a tag on it. like a picture now would usually just be unlabbeled data but it becomes labelled when it has something like a tag showing what the data is all about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf1da1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
